#
#
5 most common words:  [('the', 764307), (',', 628940), ('.', 439969), ('of', 390053), ('and', 303787)]
5 least common words:  [('2-3.1.4.0-21-2.1.4.0-2', 1), ("'barking", 1), ('breeding/nursing', 1), ('stealers', 1), ('postweaning', 1)]
Dict of 'the' should inversly map back to 'the':  the
Dict of 'person' should inversly map back to 'person':  person
Size of vocabulary is 150000 and specified max is 150000
#
# 
 Tokenizer sanity check:
['This is a test.', 'Another test.']
{'input_ids': tensor([[  2,  35,  14,  11, 968,   6,   3],
        [  2, 153, 968,   6,   3,   0,   0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 0, 0]])}
# 

#
Dataset loading sanity check: 

Length of training dataset is 147059
Length of validation dataset is 17874
First batch from training dataloader (no shuffle): 

{'text': ['Anatomy', 'Anatomy (Greek anatomē, “dissection”) is the branch of biology concerned with the study of the structure of organisms and their parts.  Anatomy is a branch of natural science dealing with the structural organization of living things.  It is an old science, having its beginnings in prehistoric times.  Anatomy is inherently tied to embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and long (evolution) timescales.  Human anatomy is one of the basic essential sciences of medicine.']}
#

Model construction sanity check with a vector: 

100 dimensional tensor input to embeedding:
 tensor([[ 0.6059, -0.0325,  0.1313,  ..., -0.4039, -0.4546, -0.6167],
        [ 0.6124, -0.0320,  0.1085,  ..., -0.4005, -0.4754, -0.6079],
        [ 0.6146,  0.0098,  0.1156,  ..., -0.4451, -0.4704, -0.5845],
        ...,
        [ 0.7181, -0.0524, -0.2675,  ..., -0.3147, -0.8170, -0.4815],
        [ 0.6976,  0.3386, -0.0579,  ..., -0.7560, -0.6398, -0.3154],
        [ 0.6343,  0.2318,  0.1253,  ..., -0.6768, -0.4693, -0.4498]],
       grad_fn=<AddmmBackward0>)
Vocab_size of 30000 with the embedding:
 torch.Size([100, 30000])

#

Device: cuda
At epoch 0, batch 0, loss = 11.933
At epoch 0, batch 1500, loss = 6.720
At epoch 0, batch 3000, loss = 6.342
At epoch 0, batch 4500, loss = 5.945
At epoch 0, batch 6000, loss = 6.155
At epoch 0, batch 7500, loss = 5.783
At epoch 0, batch 9000, loss = 5.937

#
Perplexity for the epoch is: 338.380
#
At epoch 1, batch 0, loss = 5.906
At epoch 1, batch 1500, loss = 5.598
At epoch 1, batch 3000, loss = 5.856
At epoch 1, batch 4500, loss = 5.706
At epoch 1, batch 6000, loss = 5.652
At epoch 1, batch 7500, loss = 5.449
At epoch 1, batch 9000, loss = 5.684

#
Perplexity for the epoch is: 277.636
#

#Saving to ..
#
The 5 best results for following sentence 'She lives in San' is: 

francisco 7.856065273284912
diego 6.313863754272461
and 5.571992874145508
<UNK> 5.250258445739746
cristóbal 5.010011196136475


The 5 best results for following sentence 'The lecture is about to' is: 

the 5.453012466430664
be 5.072028160095215
have 4.029440402984619
<UNK> 3.77604079246521
a 3.740055799484253