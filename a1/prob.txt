#
#
5 most common words:  [('the', 764307), (',', 628940), ('.', 439969), ('of', 390053), ('and', 303787)]
5 least common words:  [('2-3.1.4.0-21-2.1.4.0-2', 1), ("'barking", 1), ('breeding/nursing', 1), ('stealers', 1), ('postweaning', 1)]
Dict of 'the' should inversly map back to 'the':  the
Dict of 'person' should inversly map back to 'person':  person
Size of vocabulary is 150000 and specified max is 150000
#
# 
 Tokenizer sanity check:
['This is a test.', 'Another test.']
{'input_ids': tensor([[  2,  35,  14,  11, 968,   6,   3],
        [  2, 153, 968,   6,   3,   0,   0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 0, 0]])}
# 

#
Dataset loading sanity check: 

Length of training dataset is 147059
Length of validation dataset is 17874
First batch from training dataloader (no shuffle): 

{'text': ['Anatomy', 'Anatomy (Greek anatomē, “dissection”) is the branch of biology concerned with the study of the structure of organisms and their parts.  Anatomy is a branch of natural science dealing with the structural organization of living things.  It is an old science, having its beginnings in prehistoric times.  Anatomy is inherently tied to embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and long (evolution) timescales.  Human anatomy is one of the basic essential sciences of medicine.']}
#

Model construction sanity check with a vector: 

100 dimensional tensor input to embeedding:
 tensor([[-0.3757, -0.2407,  0.5750,  ...,  0.2219, -0.2043, -0.4492],
        [ 0.0024, -0.0539,  0.6177,  ...,  0.5263, -0.3874, -0.3299],
        [-0.6053, -0.2858,  0.4203,  ...,  0.2319, -0.1310, -0.3087],
        ...,
        [-0.2859, -0.2449,  0.6768,  ...,  0.1557, -0.2210, -0.5723],
        [-0.5165, -0.3138,  0.5660,  ...,  0.0984, -0.1342, -0.5048],
        [-0.4114, -0.2618,  0.5776,  ...,  0.1833, -0.1852, -0.4713]],
       grad_fn=<AddmmBackward0>)
Vocab_size of 30000 with the embedding:
 torch.Size([100, 30000])

#

Device: cuda
At epoch 0, batch 0, loss = 11.935
At epoch 0, batch 1500, loss = 6.488
At epoch 0, batch 3000, loss = 6.239
At epoch 0, batch 4500, loss = 6.077
At epoch 0, batch 6000, loss = 5.840
At epoch 0, batch 7500, loss = 5.934
At epoch 0, batch 9000, loss = 6.016
#
Perplexity for the epoch is: 341.556
#
At epoch 1, batch 0, loss = 5.817
At epoch 1, batch 1500, loss = 5.874
At epoch 1, batch 3000, loss = 5.664
At epoch 1, batch 4500, loss = 5.780
At epoch 1, batch 6000, loss = 5.645
At epoch 1, batch 7500, loss = 5.568
At epoch 1, batch 9000, loss = 5.571
#
Perplexity for the epoch is: 280.029
#

#Saving to ..
#
The 5 best results for following sentence 'She lives in San' is: 

francisco 8.536397933959961
diego 7.450388431549072
in 5.6961822509765625
suu 5.609806537628174
cristóbal 5.362128257751465


The 5 best results for following sentence 'The lecture is about to' is: 

be 6.034632682800293
the 5.948526382446289
have 4.932125568389893
a 4.2937798500061035
make 3.754843235015869