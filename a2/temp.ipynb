{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4e18909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9091f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RoPE implementation (copied and simplified from HuggingFace). ####\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, rope_rotations, unsqueeze_dim=1):\n",
    "    \"\"\"Applies precomputed RoPE rotations to the query and key representations.\"\"\"\n",
    "    assert(q.shape == k.shape)\n",
    "    assert(len(q.shape) == 4)\n",
    "    cos, sin = rope_rotations\n",
    "    assert(q.shape[2] == cos.shape[1])\n",
    "    assert(q.shape[3] == cos.shape[2])    \n",
    "    q_type, k_type = q.dtype, k.dtype\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed.to(q_type), k_embed.to(k_type)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class A2RotaryEmbedding(nn.Module):\n",
    "    \"\"\"RoPE position representation for use in Transformer attention.\"\"\"\n",
    "\n",
    "    def __init__(self, config, device=None):\n",
    "        super().__init__()\n",
    "        rope_theta = config.rope_theta\n",
    "        head_dim = config.hidden_size // config.num_attention_heads\n",
    "        partial_rotary_factor = 1.0\n",
    "        dim = int(head_dim * partial_rotary_factor)\n",
    "        self.inv_freq = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        position_ids = torch.arange(0, x.shape[1], device=x.device).unsqueeze(0)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "            return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "732cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2ModelConfig(PretrainedConfig):\n",
    "    \"\"\"Configuration object that stores hyperparameters that define the Transformer language model.\"\"\"\n",
    "    def __init__(self, vocab_size=None, hidden_size=None, intermediate_size=None, num_attention_heads=None, \n",
    "                 num_hidden_layers=None,\n",
    "                 rope_theta=None, hidden_act='silu', max_position_embeddings=None, rms_norm_eps=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.rope_theta = rope_theta\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f01ed3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2MLP(nn.Module):\n",
    "    \"\"\"The MLP layer of the Transformer. Uses the SwiGLU architecture.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert(config.hidden_act == 'silu')\n",
    "        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.linear2 = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.linear3 = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        x1 = self.act(self.linear1(hidden_states))\n",
    "        x2 = self.linear2(hidden_states)\n",
    "\n",
    "        return self.linear3(torch.mul(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1e2f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional, since you can use PyTorch's RMSNorm.\n",
    "class A2RMSNorm(nn.Module):\n",
    "    \"\"\"RMS layer normalization.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Use config.rms_norm_eps\n",
    "        # TODO: initalize weights here\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6259000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2Attention(nn.Module):\n",
    "    \"\"\"The multi-head attention layer of the Transformer. Uses standard scaled dot-product attention with causal masking.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dims = config.hidden_size\n",
    "        self.heads = config.num_attention_heads\n",
    "        #self.text = config.vocab_size\n",
    "        self.dims_head = self.dims // self.heads\n",
    "\n",
    "        # self.dims_head * self.heads => self.dims\n",
    "        self.Wq = nn.Linear(self.dims, self.dims_head * self.heads, bias=False)\n",
    "        self.Wk = nn.Linear(self.dims, self.dims_head * self.heads, bias=False)\n",
    "        self.Wv = nn.Linear(self.dims, self.dims_head * self.heads, bias=False)\n",
    "\n",
    "        self.Wo = nn.Linear(self.dims_head * self.heads, self.dims)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.dims)\n",
    "        self.norm2 = nn.LayerNorm(self.dims)\n",
    "\n",
    "    def forward(self, hidden_states, rope_rotations):\n",
    "\n",
    "        batch = hidden_states.size(0)\n",
    "        text = hidden_states.size(1)\n",
    "\n",
    "        q = self.Wq(hidden_states)\n",
    "        k = self.Wk(hidden_states)\n",
    "        v = self.Wv(hidden_states)\n",
    "\n",
    "        q = self.norm1(q)\n",
    "        k = self.norm2(k)\n",
    "\n",
    "        q = q.view(batch, text, self.heads, self.dims_head).transpose(1,2)\n",
    "        k = k.view(batch, text, self.heads, self.dims_head).transpose(1,2)\n",
    "        v = v.view(batch, text, self.heads, self.dims_head).transpose(1,2)\n",
    "\n",
    "        q, k = apply_rotary_pos_emb(q, k, rope_rotations)\n",
    "\n",
    "        y = nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "\n",
    "        return self.Wo(y.transpose(1,2).reshape(batch, text, self.dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5691c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2DecoderLayer(nn.Module):\n",
    "    \"\"\"A complete Transformer decoder layer.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.MLP = A2MLP(config)\n",
    "        self.MHA = A2Attention(config)\n",
    "        self.a1 = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True)\n",
    "        self.a2 = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True)\n",
    "\n",
    "    def forward(self, hidden_states, rope_rotations):\n",
    "\n",
    "        x = self.a1(self.MHA(hidden_states, rope_rotations))\n",
    "\n",
    "        x1 = x + hidden_states\n",
    "\n",
    "        x = self.a2(self.MLP(hidden_states))\n",
    "\n",
    "        return x1 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "243ee48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2Transformer(PreTrainedModel):\n",
    "    \"\"\"A language model based on the Transformer architecture.\"\"\"\n",
    "    \n",
    "    config_class = A2ModelConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dims = config.hidden_size\n",
    "        self.rotary_emb = A2RotaryEmbedding(config)\n",
    "        self.embeddings = torch.nn.Embedding(num_embeddings=config.vocab_size,\n",
    "                                            embedding_dim=config.embedding_dims)\n",
    "        \n",
    "        self.transformers = nn.ModuleList()\n",
    "        for _ in range(config.num_hidden_layers):\n",
    "            self.transformers.append(A2DecoderLayer(config))\n",
    "\n",
    "        self.rms = nn.RMSNorm(self.dims)\n",
    "\n",
    "        self.linear = nn.Linear(self.dims, config.vocab_size, bias=False)\n",
    "\n",
    "        self.sm = nn.Softmax(dim=-1)\n",
    "\n",
    "        # This line should be called after you have set up all components.\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        rope_rotations = self.rotary_emb(input_ids) # pass this to all the transformer decoder layers\n",
    "\n",
    "        x = self.embeddings(input_ids)\n",
    "\n",
    "        for decoder in self.transformers:\n",
    "            x = decoder(x, rope_rotations)\n",
    "\n",
    "\n",
    "        return self.linear(self.rms(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7515329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, nltk, pickle\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "from transformers import BatchEncoding, PretrainedConfig, PreTrainedModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import sys, time, os\n",
    "\n",
    "###\n",
    "### Part 1. Tokenization.\n",
    "###\n",
    "def lowercase_tokenizer(text):\n",
    "    return [t.lower() for t in nltk.word_tokenize(text)]\n",
    "\n",
    "def build_tokenizer(train_file, tokenize_fun=lowercase_tokenizer, max_voc_size=None, model_max_length=None,\n",
    "                    pad_token='<PAD>', unk_token='<UNK>', bos_token='<BOS>', eos_token='<EOS>'):\n",
    "    \"\"\" Build a tokenizer from the given file.\n",
    "\n",
    "        Args:\n",
    "             train_file:        The name of the file containing the training texts.\n",
    "             tokenize_fun:      The function that maps a text to a list of string tokens.\n",
    "             max_voc_size:      The maximally allowed size of the vocabulary.\n",
    "             model_max_length:  Truncate texts longer than this length.\n",
    "             pad_token:         The dummy string corresponding to padding.\n",
    "             unk_token:         The dummy string corresponding to out-of-vocabulary tokens.\n",
    "             bos_token:         The dummy string corresponding to the beginning of the text.\n",
    "             eos_token:         The dummy string corresponding to the end the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: build the vocabulary, possibly truncating it to max_voc_size if that is specified.\n",
    "    # Then return a tokenizer object (implemented below).\n",
    "    str_to_int = {pad_token: 0, unk_token: 1, bos_token: 2, eos_token:3}\n",
    "    int_to_str = {0: pad_token, 1: unk_token, 2: bos_token, 3: eos_token}\n",
    "    voc_len = 4 # current length\n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for paragraph in file:\n",
    "            tokens = tokenize_fun(paragraph)\n",
    "            if model_max_length:\n",
    "                tokens = tokens[:model_max_length]\n",
    "\n",
    "            word_counter.update(tokens) #just inserts and counts the words in the list\n",
    "\n",
    "    most_common_tokens = []\n",
    "    if max_voc_size:\n",
    "        max_other_tokens = max_voc_size - len(str_to_int)\n",
    "        most_common_tokens = [token for token, count in word_counter.most_common(max_other_tokens)]\n",
    "    else:\n",
    "        most_common_tokens = list(word_counter.keys())\n",
    "\n",
    "    for token in most_common_tokens:\n",
    "        str_to_int[token] = voc_len\n",
    "        int_to_str[voc_len] = token\n",
    "        voc_len += 1\n",
    "\n",
    "    print(\"#\\n5 most common words: \", word_counter.most_common(5))\n",
    "    print(\"5 least common words: \", word_counter.most_common()[-5:])\n",
    "    print(\"Dict of 'the' should inversly map back to 'the': \", int_to_str[str_to_int[\"the\"]]) \n",
    "    print(\"Dict of 'person' should inversly map back to 'person': \", int_to_str[str_to_int[\"person\"]]) \n",
    "    print(f\"Size of vocabulary is {len(str_to_int)} and specified max is {max_voc_size}\\n#\")\n",
    "\n",
    "    return A1Tokenizer(str_to_int, int_to_str, {'pad_token': pad_token,\n",
    "                                                'unk_token': unk_token, \n",
    "                                                'bos_token': bos_token, \n",
    "                                                'eos_token': eos_token}, model_max_length)\n",
    "\n",
    "\n",
    "class A1Tokenizer:\n",
    "    \"\"\"A minimal implementation of a tokenizer similar to tokenizers in the HuggingFace library.\"\"\"\n",
    "\n",
    "    def __init__(self, str_to_int, int_to_str, special_tokens, model_max_length):\n",
    "        # TODO: store all values you need in order to implement __call__ below.\n",
    "        self.pad_token_id = str_to_int[special_tokens['pad_token']]   # Compulsory attribute.\n",
    "        self.model_max_length = model_max_length # Needed for truncation.\n",
    "        self.str_to_int = str_to_int\n",
    "        self.int_to_str = int_to_str \n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    def __call__(self, texts, truncation=False, padding=False, return_tensors=\"pt\"):\n",
    "        \"\"\"Tokenize the given texts and return a BatchEncoding containing the integer-encoded tokens.\n",
    "           \n",
    "           Args:\n",
    "             texts:           The texts to tokenize.\n",
    "             truncation:      Whether the texts should be truncated to model_max_length.\n",
    "             padding:         Whether the tokenized texts should be padded on the right side.\n",
    "             return_tensors:  If None, then return lists; if 'pt', then return PyTorch tensors.\n",
    "\n",
    "           Returns:\n",
    "             A BatchEncoding where the field `input_ids` stores the integer-encoded texts.\n",
    "        \"\"\"\n",
    "        if return_tensors and return_tensors != 'pt':\n",
    "            raise ValueError('Should be pt')\n",
    "        \n",
    "        # TODO: Your work here is to split the texts into words and map them to integer values.\n",
    "        # \n",
    "        # - If `truncation` is set to True, the length of the encoded sequences should be \n",
    "        #   at most self.model_max_length.\n",
    "        # - Encoded sequences should start with the beginning-of-sequence dummy; non-truncated\n",
    "        #   sequences should end with the end-of-sequence dummy; out-of-vocabulary tokens should\n",
    "        #   be encoded with the 'unknown' dummy.\n",
    "        # - If `padding` is set to True, then all the integer-encoded sequences should be of the\n",
    "        #   same length. That is: the shorter sequences should be \"padded\" by adding dummy padding\n",
    "        #   tokens on the right side.\n",
    "        # - If `return_tensors` is undefined, then the returned `input_ids` should be a list of lists.\n",
    "        #   Otherwise, if `return_tensors` is 'pt', then `input_ids` should be a PyTorch 2D tensor.\n",
    "\n",
    "        max_seq = 0\n",
    "        encodings = []\n",
    "\n",
    "        for text in texts:\n",
    "            tokens = lowercase_tokenizer(text)\n",
    "            one_sequence = [self.str_to_int[self.special_tokens['bos_token']]]\n",
    "\n",
    "            for token in tokens:\n",
    "                token_id = self.str_to_int.get(token, self.str_to_int[self.special_tokens['unk_token']])\n",
    "                one_sequence.append(token_id)\n",
    "\n",
    "            one_sequence.append(self.str_to_int[self.special_tokens['eos_token']])\n",
    "\n",
    "            if truncation and self.model_max_length:\n",
    "                one_sequence = one_sequence[:self.model_max_length]\n",
    "\n",
    "            max_seq = max(max_seq, len(one_sequence)) #might not be used\n",
    "            encodings.append(one_sequence)\n",
    "\n",
    "        if padding:\n",
    "            pad_id = self.str_to_int[self.special_tokens['pad_token']]\n",
    "            encodings = [seq + [pad_id] * (max_seq - len(seq)) for seq in encodings]\n",
    "            \n",
    "\n",
    "        # TODO: Return a BatchEncoding where input_ids stores the result of the integer encoding.\n",
    "        # Optionally, if you want to be 100% HuggingFace-compatible, you should also include an \n",
    "        # attention mask of the same shape as input_ids. In this mask, padding tokens correspond\n",
    "        # to the the value 0 and real tokens to the value 1.\n",
    "        masks = [[1 if t != self.str_to_int[self.special_tokens['pad_token']] else 0 for t in seq] for seq in encodings]\n",
    "\n",
    "        if return_tensors == 'pt':\n",
    "            return BatchEncoding({'input_ids': torch.tensor(encodings), \n",
    "                             'attention_mask': torch.tensor(masks)})\n",
    "    \n",
    "        return BatchEncoding({'input_ids': encodings, \n",
    "                             'attention_mask': masks})\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.str_to_int)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the tokenizer to the given file.\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_file(filename):\n",
    "        \"\"\"Load a tokenizer from the given file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "55589210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hs = 32\n",
    "\n",
    "config = A2ModelConfig(\n",
    "    vocab_size=150000,\n",
    "    hidden_size=hs,\n",
    "    max_position_embeddings=100000,\n",
    "    rms_norm_eps=0.001,\n",
    "    num_attention_heads=2,\n",
    "    rope_theta=2,\n",
    "    hidden_act='silu',\n",
    "    intermediate_size=64,\n",
    "    num_hidden_layers=2,\n",
    "    embedding_dims=hs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed87d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "5 most common words:  [('the', 772302), (',', 635406), ('.', 445685), ('of', 393855), ('and', 307140)]\n",
      "5 least common words:  [('2-3.1.4.0-21-2.1.4.0-2', 1), (\"'barking\", 1), ('breeding/nursing', 1), ('stealers', 1), ('postweaning', 1)]\n",
      "Dict of 'the' should inversly map back to 'the':  the\n",
      "Dict of 'person' should inversly map back to 'person':  person\n",
      "Size of vocabulary is 150000 and specified max is 150000\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = A2Transformer(config)\n",
    "tokenizer = build_tokenizer(\"train.txt\", lowercase_tokenizer, config.vocab_size, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4325c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArguments:\n",
    "    def __init__(self, lr, epochs, batch_size):\n",
    "        self.optim = 'adamw_torch'\n",
    "        self.eval_strategy = 'epoch'\n",
    "        self.use_cpu = False\n",
    "        self.no_cuda = False\n",
    "        self.learning_rate = lr\n",
    "        self.num_train_epochs = epochs\n",
    "        self.per_device_train_batch_size = batch_size\n",
    "        self.per_device_eval_batch_size = batch_size\n",
    "        self.output_dir = \".\"\n",
    "\n",
    "class A1Trainer:\n",
    "    \"\"\"A minimal implementation similar to a Trainer from the HuggingFace library.\"\"\"\n",
    "\n",
    "    def __init__(self, model, args, train_dataset, eval_dataset, tokenizer):\n",
    "        \"\"\"Set up the trainer.\n",
    "           \n",
    "           Args:\n",
    "             model:          The model to train.\n",
    "             args:           The training parameters stored in a TrainingArguments object.\n",
    "             train_dataset:  The dataset containing the training documents.\n",
    "             eval_dataset:   The dataset containing the validation documents.\n",
    "             eval_dataset:   The dataset containing the validation documents.\n",
    "             tokenizer:      The tokenizer.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        assert(args.optim == 'adamw_torch')\n",
    "        assert(args.eval_strategy == 'epoch')\n",
    "\n",
    "    def select_device(self):\n",
    "        \"\"\"Return the device to use for training, depending on the training arguments and the available backends.\"\"\"\n",
    "        if self.args.use_cpu:\n",
    "            return torch.device('cpu')\n",
    "        if not self.args.no_cuda and torch.cuda.is_available():\n",
    "            return torch.device('cuda')\n",
    "        if torch.mps.is_available():\n",
    "            return torch.device('mps')\n",
    "        return torch.device('cpu')\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        args = self.args\n",
    "\n",
    "        device = self.select_device()\n",
    "        print('Device:', device)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        loss_func = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "\n",
    "        # TODO: Relevant arguments: at least args.learning_rate, but you can optionally also consider\n",
    "        # other Adam-related hyperparameters here.\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "        # TODO: Relevant arguments: args.per_device_train_batch_size, args.per_device_eval_batch_size\n",
    "        train_loader = DataLoader(self.train_dataset, \n",
    "                                  batch_size=args.per_device_train_batch_size,\n",
    "                                  shuffle=True)\n",
    "        val_loader = DataLoader(self.eval_dataset, \n",
    "                                batch_size=args.per_device_eval_batch_size,\n",
    "                                shuffle=True)\n",
    "        \n",
    "        # TODO: Your work here is to implement the training loop.\n",
    "        self.model.train()\n",
    "        for epoch in range(args.num_train_epochs):\n",
    "            step = 0\n",
    "            for batch in train_loader:\n",
    "                #       PREPROCESSING AND FORWARD PASS:\n",
    "                encodings = self.tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "                input_ids = encodings['input_ids']\n",
    "\n",
    "                X = input_ids[:, :-1]\n",
    "                Y = input_ids[:, 1:]\n",
    "\n",
    "                #       put X and Y onto the GPU (or whatever device you use)\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "\n",
    "                #       apply the model to X\n",
    "                logit_results = self.model(X)\n",
    "\n",
    "                #       compute the loss for the model output and Y\n",
    "                loss = loss_func(logit_results.reshape(-1, logit_results.size(-1)), Y.reshape(-1))\n",
    "\n",
    "                if step % 1500 == 0:\n",
    "                    print(f\"At epoch {epoch}, batch {step}, loss = {loss.item():.3f}\", flush=True)\n",
    "                step +=1\n",
    "\n",
    "                #       BACKWARD PASS AND MODEL UPDATE:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            self.compute_perplexity(val_loader, loss_func, device) # computes per epoch now\n",
    "\n",
    "        print(f'\\n#Saving to {args.output_dir}.\\n#')\n",
    "        self.model.save_pretrained(\"trained_model\")\n",
    "    \n",
    "\n",
    "    def compute_perplexity(self, val_loader, loss_func, device):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "                total_loss = 0.0\n",
    "                total_tokens = 0\n",
    "                pad_id = self.tokenizer.pad_token_id\n",
    "                for batch in val_loader:\n",
    "                    enc = self.tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "                    input_ids = enc['input_ids'].to(device)\n",
    "                    attn = enc['attention_mask'].to(device)\n",
    "\n",
    "                    X = input_ids[:, :-1]\n",
    "                    Y = input_ids[:, 1:]\n",
    "                    valid = attn[:, 1:]  # mask for Y to exclude padding tokens\n",
    "\n",
    "                    logits = self.model(X)\n",
    "                    loss = loss_func(logits.reshape(-1, logits.size(-1)), Y.reshape(-1))\n",
    "\n",
    "                    num_valid = valid.sum().item() #since 1 is non-padding token, summing here gives us all non-padding tokens\n",
    "                    total_loss += loss.item() * num_valid\n",
    "                    total_tokens += num_valid\n",
    "\n",
    "                perplexity = float(np.exp(total_loss / total_tokens))\n",
    "                print(f\"#\\nPerplexity for the epoch is: {perplexity:.3f}\\n#\")\n",
    "                self.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, max_length=None, temperature=None, topk=None):\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits = model(tokenizer(prompt, padding=True).input_ids)\n",
    "\n",
    "        ntl_logits = torch.exp(logits[0, -1, :] * (1 - temperature))\n",
    "    \n",
    "        \n",
    "        if topk:\n",
    "            (ntl_logits, ntl_token) = torch.topk(ntl_logits, topk)\n",
    "            category = torch.distributions.Categorical(logits=ntl_logits)\n",
    "            prompt[0] = prompt[0] + \" \" + tokenizer.int_to_str[ntl_token[category.sample().item()].item()]\n",
    "\n",
    "        else:\n",
    "            category = torch.distributions.Categorical(logits=ntl_logits)\n",
    "            prompt[0] = prompt[0] + \" \" + tokenizer.int_to_str[category.sample().item()]\n",
    "\n",
    "    \n",
    "    return prompt[0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "6125e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(0.01, 10, 16)\n",
    "\n",
    "trainer = A1Trainer(model, args, \"train.txt\", \"val.txt\", tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7f768a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he lives in san dalmatian gl√¶saria dalmatian kaye-smith pinot lenn northwestwards coppola buzzy sainte-marie\n"
     ]
    }
   ],
   "source": [
    "texts = [\"he lives in san\"]\n",
    "\n",
    "print(generate(model, texts, max_length=10, temperature=0, topk=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
